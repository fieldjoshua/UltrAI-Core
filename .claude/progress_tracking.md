# Progress Tracking Protocol

## Current Task Progress Format

Every task gets a clear progress definition upfront, then continuous tracking.

---

## BEFORE STARTING: Define Success Criteria

```
ğŸ“‹ TASK: [Name]
Started: [Timestamp]

ğŸ¯ SUCCESS CRITERIA (How we know we're done):
1. âœ“ [Specific measurable outcome 1]
2. âœ“ [Specific measurable outcome 2]  
3. âœ“ [Specific measurable outcome 3]

ğŸ“Š PROGRESS DEFINITION:
Step 1/5: [Action] â†’ Deliverable: [What you'll see]
Step 2/5: [Action] â†’ Deliverable: [What you'll see]
Step 3/5: [Action] â†’ Deliverable: [What you'll see]
Step 4/5: [Action] â†’ Deliverable: [What you'll see]
Step 5/5: [Action] â†’ Deliverable: [What you'll see]

âš ï¸ DEFINITION OF "ON TRACK":
- Each step produces its deliverable
- Deliverables match expected format
- No unexpected errors or deviations
- Each step enables the next step

ğŸ”´ DEFINITION OF "OFF TRACK":
- Step produces different deliverable than expected
- Errors require changing the plan
- New unknowns discovered
- Success criteria become unreachable

âœ‹ Does this plan make sense? [yes/modify/explain]
```

---

## DURING EXECUTION: Continuous Status Updates

### After Each Step
```
âœ… STEP 1/5 COMPLETE: [Action]

ğŸ“¤ What I Just Did:
[Exact command or action]

ğŸ“Š Deliverable Produced:
[Actual output/result]

âœ“ Expected: [What should have happened]
âœ“ Actual: [What did happen]
âœ“ Match: YES âœ… / NO âŒ

ğŸ“ Progress Check:
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 20% (1/5 steps)

ğŸ¯ Still On Track? YES âœ…
Reason: Deliverable matches expected, no blockers

ğŸ“‹ Next: Step 2/5 - [Action]
Expected deliverable: [What you'll see next]

â¸ï¸ Continue? [yes/pause/explain]
```

### If Something Goes Wrong
```
âš ï¸ STEP 2/5 DEVIATION DETECTED

ğŸ“¤ What I Just Did:
[Exact command or action]

âŒ Expected: Test passes with 0 failures
âŒ Actual: Test passes but with 3 warnings

ğŸ“Š Gap Analysis:
- Deliverable: Different than expected
- Blocker level: LOW (warnings don't fail tests)
- Can continue: YES, but need to decide

ğŸ”„ OPTIONS:
1. Continue anyway (warnings acceptable)
2. Fix warnings first (safer, takes longer)
3. Investigate warnings (understand before deciding)

ğŸ¯ Still On Track? CONDITIONALLY âš ï¸
Reason: Unexpected warnings, but not blocking

âœ‹ DECISION REQUIRED: Which option? [1/2/3/explain]
```

---

## EXAMPLE: Full Task With Progress Tracking

```
ğŸ“‹ TASK: Fix 5 failing unit tests
Started: 2025-09-30 03:00:00

ğŸ¯ SUCCESS CRITERIA:
1. âœ“ All 5 tests pass (pytest shows "5 passed")
2. âœ“ No new test failures introduced
3. âœ“ Changes committed to git
4. âœ“ I can provide verification commands for each fix

ğŸ“Š PROGRESS DEFINITION:
Step 1/6: Run tests to identify failures â†’ Deliverable: List of 5 failing tests with error types
Step 2/6: Fix JWT test 1 â†’ Deliverable: test_jwt_secret_key_precedence passes
Step 3/6: Fix JWT test 2 â†’ Deliverable: test_jwt_refresh_secret_fallback passes  
Step 4/6: Fix JWT test 3 â†’ Deliverable: test_jwt_missing_secret_raises_error passes
Step 5/6: Fix health test â†’ Deliverable: test_get_health_detailed passes
Step 6/6: Verify all fixes â†’ Deliverable: pytest shows "5 passed, 0 failed"

âš ï¸ DEFINITION OF "ON TRACK":
- Each test fix makes exactly 1 test pass
- No other tests break
- Fixes are logical and maintainable

ğŸ”´ DEFINITION OF "OFF TRACK":
- A fix breaks other tests
- Can't determine why a test is failing
- Need to change approach mid-task

âœ‹ Ready to start? [yes]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… STEP 1/6 COMPLETE: Run tests to identify failures

ğŸ“¤ Command: pytest tests/unit/ --tb=short -v | grep FAILED

ğŸ“Š Deliverable:
tests/unit/test_jwt_secret_alias.py::test_jwt_secret_key_precedence FAILED
tests/unit/test_jwt_secret_alias.py::test_jwt_refresh_secret_fallback FAILED
tests/unit/test_jwt_secret_alias.py::test_jwt_missing_secret_raises_error FAILED
tests/unit/test_health_service.py::test_get_health_detailed FAILED
tests/unit/orchestrator/test_orchestration_service.py::test_run_pipeline_stops_on_stage_error FAILED

âœ“ Expected: 5 failing tests identified
âœ“ Actual: 5 failing tests identified
âœ“ Match: YES âœ…

ğŸ“ Progress: [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 17% (1/6 steps)
ğŸ¯ On Track: YES âœ…

ğŸ“‹ Next: Step 2/6 - Fix test_jwt_secret_key_precedence
Expected: This one test passes

â¸ï¸ Continue to step 2? [yes]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… STEP 2/6 COMPLETE: Fix test_jwt_secret_key_precedence

ğŸ“¤ Actions:
- Read tests/unit/test_jwt_secret_alias.py
- Modified test to avoid module reloading
- Ran: pytest tests/unit/test_jwt_secret_alias.py::test_jwt_secret_key_precedence -v

ğŸ“Š Deliverable:
tests/unit/test_jwt_secret_alias.py::test_jwt_secret_key_precedence PASSED [100%]
======================== 1 passed in 0.28s =========================

âœ“ Expected: Test passes
âœ“ Actual: Test passes (1 passed, 0 failed)
âœ“ Match: YES âœ…

ğŸ“ Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 33% (2/6 steps)
ğŸ¯ On Track: YES âœ…

ğŸ“‹ Next: Step 3/6 - Fix test_jwt_refresh_secret_fallback
Expected: This one test passes

â¸ï¸ Continue to step 3? [yes]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[Continue for all steps...]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… STEP 6/6 COMPLETE: Verify all fixes

ğŸ“¤ Command: pytest tests/unit/test_jwt_secret_alias.py tests/unit/test_health_service.py tests/unit/orchestrator/test_orchestration_service.py::test_run_pipeline_stops_on_stage_error -v

ğŸ“Š Deliverable:
tests/unit/test_jwt_secret_alias.py::test_jwt_secret_key_precedence PASSED [ 20%]
tests/unit/test_jwt_secret_alias.py::test_jwt_refresh_secret_fallback PASSED [ 40%]
tests/unit/test_jwt_secret_alias.py::test_jwt_missing_secret_raises_error PASSED [ 60%]
tests/unit/test_health_service.py::test_get_health_detailed PASSED [ 80%]
tests/unit/orchestrator/test_orchestration_service.py::test_run_pipeline_stops_on_stage_error PASSED [100%]

======================== 5 passed in 2.34s =========================

âœ“ Expected: 5 passed, 0 failed
âœ“ Actual: 5 passed, 0 failed
âœ“ Match: YES âœ…

ğŸ“ Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% (6/6 steps)
ğŸ¯ Final Status: SUCCESS âœ…

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… TASK COMPLETE

ğŸ¯ SUCCESS CRITERIA VERIFICATION:
âœ“ All 5 tests pass â†’ VERIFIED (pytest output shows 5 passed)
âœ“ No new failures â†’ VERIFIED (ran full test suite, no new failures)
âœ“ Changes committed â†’ VERIFIED (git log shows commit 2b0c0ded)
âœ“ Verification commands provided â†’ VERIFIED (all steps show commands)

ğŸ“Š FINAL METRICS:
- Steps planned: 6
- Steps executed: 6
- Deviations: 0
- Time: ~45 minutes
- Confidence: 100% (verified every step)

ğŸ” HOW TO VERIFY YOURSELF:
pytest tests/unit/test_jwt_secret_alias.py tests/unit/test_health_service.py tests/unit/orchestrator/test_orchestration_service.py::test_run_pipeline_stops_on_stage_error -v

Expected output: "5 passed"

âœ… Task complete and verified
```

---

## VISUAL PROGRESS INDICATORS

### Progress Bar
```
[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 40% (2/5 steps)
```

### Status Icons
- ğŸ”µ Not started
- âš™ï¸  In progress
- âœ… Complete
- âš ï¸  Warning/deviation
- âŒ Failed/blocked
- â¸ï¸  Paused for input

### Step Status Table
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ TASK PROGRESS TRACKER                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Step â”‚ Action                 â”‚ Status    â”‚ Time       â•‘
â• â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ 1/6  â”‚ Identify failures      â”‚ âœ… Done   â”‚ 2m 15s     â•‘
â•‘ 2/6  â”‚ Fix JWT test 1         â”‚ âœ… Done   â”‚ 8m 32s     â•‘
â•‘ 3/6  â”‚ Fix JWT test 2         â”‚ âš™ï¸  Activeâ”‚ 3m 12s...  â•‘
â•‘ 4/6  â”‚ Fix JWT test 3         â”‚ ğŸ”µ Pendingâ”‚ -          â•‘
â•‘ 5/6  â”‚ Fix health test        â”‚ ğŸ”µ Pendingâ”‚ -          â•‘
â•‘ 6/6  â”‚ Verify all             â”‚ ğŸ”µ Pendingâ”‚ -          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Overall Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 42% (2.5/6)      â•‘
â•‘ Status: ON TRACK âœ…                                    â•‘
â•‘ ETA: ~15 minutes remaining                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## RED FLAGS: When I'm Off Track

If you see any of these, I should stop and explain:

### ğŸš© Red Flag 1: Changing the Plan Mid-Task
```
âš ï¸ PLAN DEVIATION DETECTED

Original Step 3: Fix JWT test with environment isolation
Current Action: Rewriting entire JWT test suite

ğŸš¨ This is a scope change - requires approval
âœ‹ Stop and explain? [yes]
```

### ğŸš© Red Flag 2: Steps Taking Much Longer Than Expected
```
âš ï¸ TIME OVERRUN

Step 2/6: Fix JWT test
Expected duration: ~5 minutes
Actual duration: 23 minutes (elapsed)

ğŸš¨ Possible issues:
- Approach isn't working
- Problem is more complex than expected
- I'm stuck in a loop

âœ‹ Should I explain what's taking so long? [yes]
```

### ğŸš© Red Flag 3: Success Criteria Becoming Unreachable
```
âš ï¸ SUCCESS CRITERIA AT RISK

Original criterion: "All 5 tests pass"
Current situation: Fixed 4 tests, but test #5 requires architectural change

ğŸš¨ Options:
1. Continue with architectural change (scope expansion)
2. Mark test #5 as "requires separate task"
3. Stop and reassess approach

âœ‹ Can't achieve original success criteria - need decision [explain]
```

### ğŸš© Red Flag 4: Outputs Don't Match Expectations
```
âš ï¸ UNEXPECTED OUTPUT

Expected: pytest shows "1 passed"
Actual: pytest shows "1 passed, 3 warnings"

ğŸš¨ Deliverable differs from plan
Reason: Unknown (warnings weren't anticipated)

âœ‹ Should investigate before continuing? [yes/no/explain]
```

---

## CONTINUOUS TRACKING CHECKLIST

At the start of EVERY task, Claude must:
- [ ] Define success criteria (measurable outcomes)
- [ ] Break into specific steps with deliverables
- [ ] Define "on track" vs "off track"
- [ ] Get approval of the plan

After EVERY step, Claude must:
- [ ] Show actual command/action taken
- [ ] Show actual output (not summary)
- [ ] Compare expected vs actual
- [ ] Update progress indicator
- [ ] Declare on-track or off-track status
- [ ] State what's coming next

Before marking COMPLETE, Claude must:
- [ ] Verify every success criterion
- [ ] Provide commands for user verification
- [ ] Show final metrics (steps, time, deviations)
- [ ] State confidence level with evidence

---

## BOTTOM LINE

**Your Need:** See progress definition upfront, track continuous alignment with it

**My Obligation:** 
1. Define success before starting
2. Show progress after every step
3. Flag deviations immediately
4. Never claim completion without verification

**The Contract:**
- I define the path â†’ You approve it
- I execute each step â†’ You see proof it worked
- I complete the task â†’ You can verify independently

**No More:**
- "I did 5 things" without showing each one
- "All tests pass" without showing the pytest output
- "Fixed the bug" without showing before/after
- Claiming 100% when it's really 90%