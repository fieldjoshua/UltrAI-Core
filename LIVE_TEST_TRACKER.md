# Live Test Tracker

**Purpose:** Track real-time test execution progress organized by functional module and test category.

**Generated by:** `scripts/track_test_progress.py`

## Organization

Tests are organized into the same 21 functional modules as the test inventory:

### Core Functionality
- **1. Core Synthesis** - Orchestration pipeline, multi-model synthesis [CRITICAL]
- **2. LLM Providers** - OpenAI, Anthropic, Google, HuggingFace adapters [CRITICAL]

### Security & Access
- **3. Authentication** - JWT, OAuth, API keys [CRITICAL]
- **4. Financial** - Billing, transactions, budgets [CRITICAL]
- **5. Rate Limiting** - Throttling, quotas, tier limits [CRITICAL]

### Infrastructure
- **6. Caching** - Redis, in-memory, cache strategies
- **7. Health & Monitoring** - Service health, metrics [CRITICAL]
- **8. Prompts & Templates** - Template rendering, variables

### Quality & Tracking
- **9. Quality Evaluation** - Response quality scoring
- **10. Token Management** - Usage tracking, cost calculation [CRITICAL]
- **11. Model Registry** - Model registration, configuration

### Data & Content
- **12. Documents** - Document processing, file handling
- **13. Analysis** - Text analysis, pattern detection

### API & Integration
- **14. API Endpoints** - Route testing, endpoint validation [CRITICAL]
- **15. Streaming** - SSE events, real-time responses
- **16. Configuration** - System config, environment setup
- **17. Database** - Repository pattern, transactions [CRITICAL]

### User Experience
- **18. UI/Frontend** - Playwright browser tests [CRITICAL]
- **19. Network** - Connectivity, external calls

### Resilience
- **20. Error Handling** - Failure scenarios, fallbacks
- **21. Miscellaneous** - Telemetry, middleware, utilities

### Test Types
- **E2E** - End-to-end workflow tests
- **Integration** - Service integration tests [CRITICAL]
- **Live** - Tests against real APIs
- **Production** - Production endpoint validation [CRITICAL]

## Usage

### Start Tracking

```bash
# Initialize tracker with output file
python scripts/track_test_progress.py --output test_progress.json
```

### Import Test Results

```bash
# Run tests and capture results
pytest tests/unit/orchestrator/ --json-report --json-report-file=results.json

# Import into tracker
python scripts/track_test_progress.py \
  --output test_progress.json \
  --import-results results.json \
  --category 1_core_synthesis
```

### View Progress

```bash
# Display current progress
python scripts/track_test_progress.py --output test_progress.json
```

### CI/CD Integration

```yaml
# Example GitHub Actions workflow
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - name: Initialize tracker
        run: python scripts/track_test_progress.py --output test_progress.json
      
      - name: Run Core Synthesis tests
        run: |
          pytest tests/unit/orchestrator/ --json-report --json-report-file=core.json
          python scripts/track_test_progress.py \
            --output test_progress.json \
            --import-results core.json \
            --category 1_core_synthesis
      
      - name: Run LLM Provider tests
        run: |
          pytest tests/test_llm_adapters*.py --json-report --json-report-file=llm.json
          python scripts/track_test_progress.py \
            --output test_progress.json \
            --import-results llm.json \
            --category 2_llm_providers
      
      - name: Display summary
        run: python scripts/track_test_progress.py --output test_progress.json
```

## Output Format

The tracker displays results in a table organized by module:

```
MODULE       CATEGORY                  DESCRIPTION                    STATUS     PASS   FAIL   SKIP   TIME    
-------------------------------------------------------------------------------------------------------------------
Core         1_core_synthesis          Core Synthesis                 Success    14     0      0      12.3s
             2_llm_providers           LLM Providers                  Running    28     1      0      45.2s
Security     3_authentication          Authentication                 Success    35     0      0      8.7s
             4_financial               Financial                      Pending    0      0      0      
             5_rate_limiting           Rate Limiting                  Success    44     0      2      15.4s
```

**Legend:**
- **Bold descriptions** = Critical test categories
- Green = Passing
- Red = Failing
- Yellow = Skipped
- Blue = Currently running

## Progress JSON Schema

```json
{
  "summary": {
    "start_time": 1234567890.0,
    "elapsed": 123.45,
    "timestamp": "2025-09-30T12:00:00",
    "categories_completed": 10,
    "categories_pending": 15,
    "categories_running": 0,
    "total_passed": 150,
    "total_failed": 2,
    "total_skipped": 5,
    "success": false
  },
  "categories": {
    "1_core_synthesis": {
      "status": "success",
      "start_time": 1234567890.0,
      "end_time": 1234567902.3,
      "duration": 12.3,
      "passed": 14,
      "failed": 0,
      "skipped": 0,
      "total": 14,
      "details": [
        {
          "test": "tests/unit/orchestrator/test_basic.py::test_success",
          "status": "passed",
          "duration": 0.123
        }
      ]
    }
  }
}
```

## Alignment with Test Inventory

The live test tracker uses the same 21-module organizational structure as [TEST_INVENTORY.md](./TEST_INVENTORY.md), ensuring consistency across:

1. **Test documentation** (TEST_INVENTORY.md)
2. **Test execution tracking** (LIVE_TEST_TRACKER.md)
3. **Test automation scripts** (scripts/track_test_progress.py)

All three use identical category names and module groupings, making it easy to:
- Find tests in the inventory
- Track their execution progress
- Organize CI/CD pipelines
- Generate consistent reports

---

**Last Updated:** 2025-09-30  
**Script Location:** `scripts/track_test_progress.py`  
**Related Docs:** [TEST_INVENTORY.md](./TEST_INVENTORY.md), [tests/README.md](./tests/README.md)