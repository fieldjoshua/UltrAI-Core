**Technical Field**

The present invention relates to the field of artificial intelligence (AI), and more particularly to systems and methods for orchestrating and analyzing responses from multiple AI models.

**Background**

In recent years, there has been a surge in the development and deployment of AI models. These models have been successfully applied to a wide range of tasks, including natural language processing, computer vision, and machine learning.

However, one of the challenges with AI models is that they can be difficult to use effectively. AI models often require specialized knowledge and expertise to train and deploy, and they can be difficult to integrate with other systems.

**Summary of the Invention**

The present invention addresses the challenges of using AI models by providing a system and method for orchestrating and analyzing responses from multiple AI models. The system and method of the present invention are designed to make it easier to use AI models and to improve the quality of the results obtained from AI models.

**Detailed Description**

The system of the present invention includes a plurality of AI models, a prompt generator, and an analyzer. The AI models are used to generate responses to a given prompt. The prompt generator is used to generate prompts for the AI models. The analyzer is used to analyze the responses from the AI models and to generate a final response.

The system of the present invention operates as follows:

1. The user inputs a prompt into the system.
2. The prompt generator generates a plurality of prompts for the AI models.
3. The AI models generate responses to the prompts.
4. The analyzer analyzes the responses from the AI models and generates a final response.

The analyzer may use a variety of techniques to analyze the responses from the AI models. For example, the analyzer may use natural language processing techniques to identify the key points in the responses. The analyzer may also use machine learning techniques to identify patterns in the responses.

The final response generated by the analyzer may be used for a variety of purposes. For example, the final response may be used to provide information to a user, to make a decision, or to control a device.

**Claims**

1. A system for orchestrating and analyzing responses from multiple AI models, comprising:

    (a) a plurality of AI models;
    (b) a prompt generator; and
    (c) an analyzer.

2. The system of claim 1, wherein the prompt generator is used to generate prompts for the AI models.

3. The system of claim 1, wherein the analyzer is used to analyze the responses from the AI models and to generate a final response.

4. The system of claim 1, wherein the analyzer uses a plurality of techniques to analyze the responses from the AI models.

5. The system of claim 4, wherein the techniques used by the analyzer include natural language processing techniques and machine learning techniques.

6. A method for orchestrating and analyzing responses from multiple AI models, comprising the steps of:

    (a) inputting a prompt into a system;
    (b) generating a plurality of prompts for the AI models using a prompt generator;
    (c) generating responses to the prompts using the AI models; and
    (d) analyzing the responses from the AI models and generating a final response using an analyzer.

7. The method of claim 6, wherein the analyzer uses a plurality of techniques to analyze the responses from the AI models.

8. The method of claim 7, wherein the techniques used by the analyzer include natural language processing techniques and machine learning techniques.

**Code Implementation**

The following is a code implementation of the system and method of the present invention:

```python
import asyncio
import logging
import os

from dotenv import load_dotenv
from openai import OpenAI
from dataclasses import dataclass
from google.generativelanguage import models, clients

load_dotenv()  # This needs to be called before accessing any env vars

@dataclass
class PromptTemplate:
    initial: str = "Please analyze the following: {prompt}"
    meta: str = "Analyze these responses and create an improved version: {responses}"
    ultra: str = "Create a final synthesis of these analyses: {responses}"
    hyper: str = "Perform a hyper-level analysis of all previous responses: {responses}"

@dataclass
class RateLimits:
    llama: int = 5
    chatgpt: int = 3
    gemini: int = 10

class TriLLMOrchestrator:
    def __init__(self,
                 api_keys: Dict[str, str],
                 prompt_templates: Optional[PromptTemplate] = None,
                 rate_limits: Optional[RateLimits] = None,
                 output_format: str = "plain",
                 ultra_engine: str = "llama"):

        print("Initializing TriLLMOrchestrator...")

        # Setup logging
        self.logger = logging.getLogger(__name__)

        # Store the prompt first so we can use it for the directory name
        self.prompt = None
        self.base_dir = os.path.join(os.getcwd(), 'responses')

        print("\nChecking API keys...")
        self.api_keys = api_keys
        print(f"OpenAI: {api_keys.get('openai', '')[:5]}...{api_keys.get('openai', '')[-4:]}")
        print(f"Google: {api_keys.get('google', '')[:5]}...{api_keys.get('google', '')[-4:]}")

        print("\nSetting up formatter...")
        self.output_format = output_format

        print("\nInitializing API clients...")
        self._initialize_clients()

        self.prompt_templates = prompt_templates or PromptTemplate()
        self.rate_limits = rate_limits or RateLimits()
        self.last_request_time = {"llama": 0, "chatgpt": 0, "gemini": 0}
        self.ultra_engine = ultra_engine

    def _get_keyword_from_prompt(self, prompt: str) -> str:
        """Extract a meaningful keyword from the prompt"""
        common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'please', 'edit', 'following'}

        words = prompt.lower().split()
        keyword = next((word for word in words if word not in common_words and len(word) > 2), 'task')

        keyword = ''.join(c for c in keyword if c.isalnum())
        return keyword[:15]

    def _initialize_clients(self):
        """Initialize API clients for each service"""
        print("Initializing Llama...")
        # Llama uses local API, no initialization needed
        print("Llama initialized successfully")

        print("Initializing OpenAI...")
        self.openai_client = OpenAI(api_key=self.api_keys["openai"])
        print("OpenAI initialized successfully")

        print("Initializing Gemini...")
        self.gemini_client = clients.LanguageServiceClient()
        print("Gemini initialized successfully")

    def _setup_directory(self):
        """Create directory for this run"""
        os.makedirs(self.run_dir, exist_ok=True)

    def formatter(self, text: str) -> str:
        """Format the output based on specified format"""
        if self.output_format == "plain":
            return text
        # Add more format options as needed
        return text

    async def _respect_rate_limit(self, service: str):
        """Ensure we don't exceed rate limits"""
        current_time = datetime.now().timestamp()
        time_since_last = current_time - self.last_request_time[service]

        if time_since_last < self.rate_limits.__dict__[service]:
            await asyncio.sleep(self.rate_limits.__dict__[service] - time_since_last)

        self.last_request_time[service] = current_time

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    async def get_llama_response(self, prompt: str) -> str:
        await self._respect_rate_limit("llama")
        try:
            response = requests.post(
                'http://localhost:11434/api/generate',
                json={
                    'model': 'llama2',
                    'prompt': prompt,
                    'stream': False
                }
            )

            if response.status_code == 200:
                result = response.json()
                return self.formatter(result['response'])
            else:
                raise Exception(f"Llama HTTP error: {response.status_code}")

        except Exception as e:
            self.logger.error(f"Error with Llama API: {str(e)}")
            raise

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
