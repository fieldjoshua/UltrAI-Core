# Document Analysis Testing Guidelines

## Overview

This document provides guidelines for implementing comprehensive tests for the Document Analysis flow in the Ultra MVP. The document analysis feature allows users to upload documents, analyze them with multiple LLM models, and retrieve structured results.

## Test Coverage Requirements

Document Analysis tests should cover the following areas:

1. **Document Upload and Processing**

   - Document upload validation
   - File type validation and sanitization
   - Document storage and retrieval
   - Chunking and preprocessing

2. **Document Analysis**

   - Analysis request submission
   - Multiple model selection and processing
   - Analysis pattern application
   - Result collation and formatting

3. **Result Retrieval and Display**

   - Result storage and retrieval
   - Result formatting and rendering
   - Caching and performance
   - Export and sharing

4. **Error Handling**
   - Invalid document handling
   - Failed analysis recovery
   - Partial result management
   - Rate limiting and quota management

## Test Implementation Guidelines

### Backend Tests

For backend document analysis tests, follow these guidelines:

#### 1. Test Organization

Organize document analysis tests into multiple files based on functionality:

- `test_document_upload.py`: Document upload and storage
- `test_document_processing.py`: Document preprocessing and chunking
- `test_document_analysis.py`: Analysis submission and processing
- `test_document_results.py`: Result retrieval and formatting

#### 2. Test Fixtures

Create reusable fixtures to simplify testing:

```python
@pytest.fixture
def sample_pdf_file():
    """Create a sample PDF file for upload testing."""
    file_path = os.path.join(os.path.dirname(__file__), "fixtures", "sample.pdf")
    with open(file_path, "rb") as f:
        pdf_content = f.read()
    return pdf_content

@pytest.fixture
def sample_text_file():
    """Create a sample text file for upload testing."""
    content = "This is a sample text document for testing document analysis."
    return content.encode("utf-8")

@pytest.fixture
def uploaded_document(client, auth_headers, sample_pdf_file):
    """Upload a document and return its document ID."""
    files = {"file": ("sample.pdf", sample_pdf_file, "application/pdf")}

    response = client.post(
        "/api/upload-document",
        files=files,
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "document_id" in response.json()

    return response.json()["document_id"]

@pytest.fixture
def analyzed_document(client, auth_headers, uploaded_document):
    """Analyze a document and return its analysis ID."""
    analysis_payload = {
        "document_id": uploaded_document,
        "selected_models": ["gpt4o", "claude37"],
        "ultra_model": "gpt4o",
        "pattern": "confidence",
        "options": {}
    }

    response = client.post(
        "/api/analyze-document",
        json=analysis_payload,
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "analysis_id" in response.json()

    return response.json()["analysis_id"]
```

#### 3. Required Tests

Implement tests for the following scenarios:

**Document Upload Tests**

```python
def test_document_upload_pdf(client, auth_headers, sample_pdf_file):
    """Test uploading a PDF document."""
    files = {"file": ("sample.pdf", sample_pdf_file, "application/pdf")}

    response = client.post(
        "/api/upload-document",
        files=files,
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "document_id" in response.json()
    assert "filename" in response.json()
    assert response.json()["filename"] == "sample.pdf"
    assert "content_type" in response.json()
    assert response.json()["content_type"] == "application/pdf"

def test_document_upload_text(client, auth_headers, sample_text_file):
    """Test uploading a text document."""
    files = {"file": ("sample.txt", sample_text_file, "text/plain")}

    response = client.post(
        "/api/upload-document",
        files=files,
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "document_id" in response.json()

def test_document_upload_invalid_type(client, auth_headers):
    """Test uploading a document with an invalid type."""
    invalid_file = b"This is not a valid document"
    files = {"file": ("invalid.exe", invalid_file, "application/octet-stream")}

    response = client.post(
        "/api/upload-document",
        files=files,
        headers=auth_headers
    )

    assert response.status_code == 400
    assert "error" in response.json()
    assert "file type" in response.json()["error"].lower()

def test_document_upload_large_file(client, auth_headers):
    """Test uploading a document that exceeds size limits."""
    # Create a large file (e.g., 11MB if limit is 10MB)
    large_file = b"0" * (11 * 1024 * 1024)
    files = {"file": ("large.txt", large_file, "text/plain")}

    response = client.post(
        "/api/upload-document",
        files=files,
        headers=auth_headers
    )

    assert response.status_code == 413  # Request Entity Too Large
    assert "error" in response.json()
    assert "size" in response.json()["error"].lower()
```

**Document Processing Tests**

```python
def test_document_retrieval(client, auth_headers, uploaded_document):
    """Test retrieving an uploaded document."""
    response = client.get(
        f"/api/documents/{uploaded_document}",
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "document_id" in response.json()
    assert response.json()["document_id"] == uploaded_document
    assert "filename" in response.json()
    assert "content_type" in response.json()
    assert "upload_time" in response.json()

def test_document_text_extraction(client, auth_headers, uploaded_document):
    """Test extracting text from an uploaded document."""
    response = client.get(
        f"/api/documents/{uploaded_document}/text",
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "text" in response.json()
    assert len(response.json()["text"]) > 0
    assert "pages" in response.json()
    assert len(response.json()["pages"]) > 0

def test_document_metadata(client, auth_headers, uploaded_document):
    """Test retrieving document metadata."""
    response = client.get(
        f"/api/documents/{uploaded_document}/metadata",
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "document_id" in response.json()
    assert "word_count" in response.json()
    assert "page_count" in response.json()
    assert "created_at" in response.json()
```

**Document Analysis Tests**

```python
def test_document_analysis_request(client, auth_headers, uploaded_document):
    """Test requesting document analysis."""
    analysis_payload = {
        "document_id": uploaded_document,
        "selected_models": ["gpt4o", "claude37"],
        "ultra_model": "gpt4o",
        "pattern": "confidence",
        "options": {}
    }

    response = client.post(
        "/api/analyze-document",
        json=analysis_payload,
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "analysis_id" in response.json()
    assert "status" in response.json()
    assert response.json()["status"] in ["pending", "processing", "completed"]

def test_document_analysis_invalid_model(client, auth_headers, uploaded_document):
    """Test analysis with invalid model selection."""
    analysis_payload = {
        "document_id": uploaded_document,
        "selected_models": ["invalid_model"],
        "ultra_model": "invalid_model",
        "pattern": "confidence",
        "options": {}
    }

    response = client.post(
        "/api/analyze-document",
        json=analysis_payload,
        headers=auth_headers
    )

    # Should return error in real mode, might succeed in mock mode
    if response.status_code != 200:
        assert response.status_code == 400
        assert "error" in response.json()
        assert "model" in response.json()["error"].lower()

def test_document_analysis_invalid_pattern(client, auth_headers, uploaded_document):
    """Test analysis with invalid pattern selection."""
    analysis_payload = {
        "document_id": uploaded_document,
        "selected_models": ["gpt4o"],
        "ultra_model": "gpt4o",
        "pattern": "invalid_pattern",
        "options": {}
    }

    response = client.post(
        "/api/analyze-document",
        json=analysis_payload,
        headers=auth_headers
    )

    # Should return error in real mode, might default in mock mode
    if response.status_code != 200:
        assert response.status_code == 400
        assert "error" in response.json()
        assert "pattern" in response.json()["error"].lower()
```

**Document Results Tests**

```python
def test_document_analysis_results(client, auth_headers, analyzed_document):
    """Test retrieving document analysis results."""
    # May need to poll for completion in real mode
    max_retries = 5
    for i in range(max_retries):
        response = client.get(
            f"/api/document-analysis/{analyzed_document}",
            headers=auth_headers
        )

        assert response.status_code == 200

        if response.json()["status"] == "completed":
            break

        time.sleep(2)  # Wait before retrying

    assert response.json()["status"] == "completed"
    assert "results" in response.json()
    assert "model_responses" in response.json()["results"]
    assert "ultra_response" in response.json()["results"]
    assert "document_metadata" in response.json()["results"]

def test_document_analysis_history(client, auth_headers, analyzed_document):
    """Test retrieving document analysis history."""
    response = client.get(
        "/api/analysis/history",
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "analyses" in response.json()

    # The analyzed document should be in the history
    analysis_ids = [a["id"] for a in response.json()["analyses"]]
    assert analyzed_document in analysis_ids

def test_document_analysis_export(client, auth_headers, analyzed_document):
    """Test exporting document analysis results."""
    response = client.get(
        f"/api/document-analysis/{analyzed_document}/export",
        headers=auth_headers
    )

    assert response.status_code == 200
    assert "content-disposition" in response.headers
    assert "attachment" in response.headers["content-disposition"]
```

### Frontend Tests

For frontend document analysis tests, use Cypress to test the complete document flow:

```javascript
describe('Document Analysis', () => {
  beforeEach(() => {
    // Login user
    cy.login('test@example.com', 'password123');

    // Visit documents page
    cy.visit('/documents');
  });

  it('should upload a document', () => {
    // Click upload button
    cy.get('[data-cy=upload-document-button]').click();

    // Upload file
    cy.get('[data-cy=file-input]').attachFile('sample.pdf');

    // Submit upload
    cy.get('[data-cy=upload-submit]').click();

    // Verify success message
    cy.get('[data-cy=toast]').should(
      'contain',
      'Document uploaded successfully'
    );

    // Verify document appears in list
    cy.get('[data-cy=document-list]').should('contain', 'sample.pdf');
  });

  it('should analyze a document', () => {
    // Upload a document first
    cy.uploadDocument('sample.pdf');

    // Click on document
    cy.get('[data-cy=document-item]').contains('sample.pdf').click();

    // Initiate analysis
    cy.get('[data-cy=analyze-button]').click();

    // Select models
    cy.get('[data-cy=model-checkbox]').first().check();

    // Select pattern
    cy.get('[data-cy=pattern-select]').select('confidence');

    // Submit analysis
    cy.get('[data-cy=submit-analysis]').click();

    // Verify analysis starts
    cy.get('[data-cy=analysis-progress]').should('be.visible');

    // Wait for analysis to complete (with timeout)
    cy.get('[data-cy=analysis-result]', { timeout: 30000 }).should(
      'be.visible'
    );

    // Verify analysis results are displayed
    cy.get('[data-cy=model-responses]').should('be.visible');
    cy.get('[data-cy=ultra-response]').should('be.visible');
  });

  it('should show document history', () => {
    // Upload and analyze a document
    cy.uploadAndAnalyzeDocument('sample.pdf', ['gpt4o'], 'confidence');

    // Go to history page
    cy.visit('/history');

    // Verify document appears in history
    cy.get('[data-cy=history-item]').should('contain', 'sample.pdf');

    // Verify analysis details
    cy.get('[data-cy=history-item]')
      .first()
      .within(() => {
        cy.get('[data-cy=document-name]').should('contain', 'sample.pdf');
        cy.get('[data-cy=analysis-pattern]').should('contain', 'confidence');
        cy.get('[data-cy=analysis-date]').should('be.visible');
      });
  });

  it('should handle document analysis errors', () => {
    // Mock server to return error during analysis
    cy.intercept('POST', '/api/analyze-document', {
      statusCode: 500,
      body: {
        status: 'error',
        error: 'Analysis service unavailable',
      },
    }).as('analyzeError');

    // Upload a document
    cy.uploadDocument('sample.pdf');

    // Try to analyze
    cy.get('[data-cy=document-item]').contains('sample.pdf').click();
    cy.get('[data-cy=analyze-button]').click();
    cy.get('[data-cy=model-checkbox]').first().check();
    cy.get('[data-cy=pattern-select]').select('confidence');
    cy.get('[data-cy=submit-analysis]').click();

    // Wait for error response
    cy.wait('@analyzeError');

    // Verify error message
    cy.get('[data-cy=error-message]').should(
      'contain',
      'Analysis service unavailable'
    );

    // Verify retry button
    cy.get('[data-cy=retry-button]').should('be.visible');
  });
});
```

## Test Execution Strategy

To ensure comprehensive document analysis testing:

1. **Start with Upload Tests**

   - Document upload is the foundation for analysis
   - Make sure files are properly validated and stored

2. **Test Document Processing**

   - Verify text extraction
   - Test metadata generation
   - Check chunking functionality

3. **Test Analysis Functionality**

   - Verify model selection
   - Test various analysis patterns
   - Check results collation

4. **Test End-to-End Flows**
