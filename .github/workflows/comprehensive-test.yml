name: Ultra Comprehensive Test Suite

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      coverage_report:
        description: 'Generate HTML coverage report'
        type: boolean
        default: false

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - uses: actions/checkout@v3

      - name: Generate cache key
        id: cache-key
        run: echo "key=deps-$(date +'%Y%m%d')-${{ hashFiles('**/requirements*.txt') }}" >> $GITHUB_OUTPUT

  backend-tests:
    needs: setup
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: [3.9, "3.10"]
        test-category: [api, auth, rate-limit, document, integration]

    services:
      redis:
        image: redis:6
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ needs.setup.outputs.cache-key }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install pytest pytest-cov pytest-xdist

    - name: Configure environment
      run: |
        echo "MOCK_MODE=true" >> $GITHUB_ENV
        echo "ENVIRONMENT=testing" >> $GITHUB_ENV
        echo "DEBUG=true" >> $GITHUB_ENV
        echo "SENTRY_DSN=" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV
        echo "ALLOWED_EXTERNAL_DOMAINS=api.openai.com,api.anthropic.com,api.mistral.ai" >> $GITHUB_ENV

        # Create required directories
        mkdir -p document_storage
        mkdir -p temp_uploads
        mkdir -p logs

    - name: Run ${{ matrix.test-category }} tests
      run: |
        cd backend

        # Set test pattern based on category
        if [ "${{ matrix.test-category }}" = "api" ]; then
          TEST_PATTERN="test_analyze_endpoint.py test_available_models_endpoint.py test_llm_request_endpoint.py test_health_endpoint.py test_api.py"
        elif [ "${{ matrix.test-category }}" = "auth" ]; then
          TEST_PATTERN="test_auth_endpoints.py test_jwt_utils.py test_auth_edge_cases.py test_e2e_auth_workflow.py"
        elif [ "${{ matrix.test-category }}" = "rate-limit" ]; then
          TEST_PATTERN="test_rate_limit_service.py test_rate_limit_middleware.py"
        elif [ "${{ matrix.test-category }}" = "document" ]; then
          TEST_PATTERN="test_e2e_analysis_flow.py test_document_upload.py"
        elif [ "${{ matrix.test-category }}" = "integration" ]; then
          TEST_PATTERN="test_orchestrator.py test_basic_orchestrator.py"
        fi

        echo "Running tests: $TEST_PATTERN"
        PYTHONPATH=$PWD:$PYTHONPATH pytest tests/$TEST_PATTERN -v --cov=. --cov-report=xml --cov-report=term --cov-append

    - name: Upload coverage for ${{ matrix.test-category }}
      if: matrix.python-version == '3.10'
      uses: actions/upload-artifact@v3
      with:
        name: coverage-${{ matrix.test-category }}
        path: backend/coverage.xml
        retention-days: 1

  frontend-tests:
    needs: setup
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: 'frontend/package-lock.json'

    - name: Install dependencies
      run: |
        cd frontend
        npm ci

    - name: Run frontend tests
      run: |
        cd frontend
        npm test -- --coverage

    - name: Upload frontend coverage
      uses: actions/upload-artifact@v3
      with:
        name: coverage-frontend
        path: frontend/coverage
        retention-days: 1

  coverage-report:
    needs: [backend-tests, frontend-tests]
    runs-on: ubuntu-latest
    if: always()

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install coverage codecov

    - name: Download all coverage reports
      uses: actions/download-artifact@v3

    - name: Combine coverage reports
      run: |
        mkdir -p .coverage_reports
        cp coverage-*/coverage.xml .coverage_reports/ || true

        # Generate combined report
        python -m coverage combine .coverage_reports/*.xml || true
        python -m coverage xml
        python -m coverage report || true

    - name: Generate HTML coverage report
      if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.coverage_report == 'true' }}
      run: |
        python -m coverage html

    - name: Upload combined coverage to CodeCov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

    - name: Upload HTML coverage report
      if: ${{ github.event_name == 'workflow_dispatch' && github.event.inputs.coverage_report == 'true' }}
      uses: actions/upload-artifact@v3
      with:
        name: coverage-html-report
        path: htmlcov
        retention-days: 7

  e2e-tests:
    needs: [backend-tests, frontend-tests]
    runs-on: ubuntu-latest
    if: ${{ always() && !contains(needs.*.result, 'failure') }}

    services:
      redis:
        image: redis:6
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        pip install pytest pytest-cov

    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: 'frontend/package-lock.json'

    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci

    - name: Configure environment
      run: |
        echo "MOCK_MODE=true" >> $GITHUB_ENV
        echo "ENVIRONMENT=testing" >> $GITHUB_ENV
        echo "DEBUG=true" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV

        # Create required directories
        mkdir -p document_storage
        mkdir -p temp_uploads
        mkdir -p logs

    - name: Start backend server
      run: |
        python -m uvicorn backend.app:app --host 127.0.0.1 --port 8085 &
        sleep 5  # Give server time to start
      env:
        MOCK_MODE: "true"
        ENVIRONMENT: "testing"
        DEBUG: "true"

    - name: Run E2E tests
      run: |
        python test_api.py --base-url http://localhost:8085 --models gpt4o,claude3opus

  lint:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort

    - name: Lint with flake8
      run: |
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=150 --statistics

    - name: Check formatting with black
      run: |
        black --check --diff .

    - name: Check import formatting with isort
      run: |
        isort --check --diff .

  security-scan:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety

    - name: Run Bandit security scan
      run: |
        bandit -r . -x ./tests,./venv -f json -o bandit-results.json || true

    - name: Check dependencies for vulnerabilities
      run: |
        safety check -r requirements.txt || true

    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      with:
        name: security-scan-results
        path: bandit-results.json
        retention-days: 7

  test-summary:
    needs: [backend-tests, frontend-tests, e2e-tests, lint, security-scan]
    runs-on: ubuntu-latest
    if: always()

    steps:
    - uses: actions/checkout@v3

    - name: Get test results
      run: |
        echo "## Ultra Test Summary" > test-summary.md
        echo "" >> test-summary.md
        echo "| Test Type | Status |" >> test-summary.md
        echo "|-----------|--------|" >> test-summary.md

        # Backend Tests
        if [[ "${{ needs.backend-tests.result }}" == "success" ]]; then
          echo "| Backend Tests | ✅ Passed |" >> test-summary.md
        else
          echo "| Backend Tests | ❌ Failed |" >> test-summary.md
        fi

        # Frontend Tests
        if [[ "${{ needs.frontend-tests.result }}" == "success" ]]; then
          echo "| Frontend Tests | ✅ Passed |" >> test-summary.md
        else
          echo "| Frontend Tests | ❌ Failed |" >> test-summary.md
        fi

        # E2E Tests
        if [[ "${{ needs.e2e-tests.result }}" == "success" ]]; then
          echo "| E2E Tests | ✅ Passed |" >> test-summary.md
        else
          echo "| E2E Tests | ❌ Failed |" >> test-summary.md
        fi

        # Lint
        if [[ "${{ needs.lint.result }}" == "success" ]]; then
          echo "| Linting | ✅ Passed |" >> test-summary.md
        else
          echo "| Linting | ❌ Failed |" >> test-summary.md
        fi

        # Security Scan
        if [[ "${{ needs.security-scan.result }}" == "success" ]]; then
          echo "| Security Scan | ✅ Passed |" >> test-summary.md
        else
          echo "| Security Scan | ⚠️ Warnings |" >> test-summary.md
        fi

        echo "" >> test-summary.md
        echo "See detailed results in the workflow run." >> test-summary.md

    - name: Create summary
      run: cat test-summary.md >> $GITHUB_STEP_SUMMARY
