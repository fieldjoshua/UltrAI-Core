from fastapi import FastAPI, HTTPException, UploadFile, File, Form, Response, Request, BackgroundTasks, Depends, Query, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ValidationError
from typing import List, Dict, Any, Optional
import asyncio
import os
import json
import uuid
import tempfile
from datetime import datetime
import sys
import traceback
import multiprocessing
import time
import psutil
from fastapi.responses import JSONResponse, PlainTextResponse
import uvicorn
import logging
try:
    import sentry_sdk
    SENTRY_AVAILABLE = True
except ImportError:
    SENTRY_AVAILABLE = False
from cachetools import TTLCache, cached
import hashlib
import shutil
from pathlib import Path

# Import error handling system
from error_handler import register_exception_handlers, error_handling_middleware

# Configure Sentry for error tracking and performance monitoring if available
if SENTRY_AVAILABLE:
    sentry_sdk.init(
        dsn="https://860c945f86e625b606babebefb04c009@o4509109008531456.ingest.us.sentry.io/4509109123350528",
        # Add data like request headers and IP for users
        send_default_pii=True,
        # Adjust this in production to reduce the volume of performance data
        traces_sample_rate=1.0,
        # Set environment based on ENVIRONMENT variable
        environment=os.getenv("ENVIRONMENT", "development"),
    )

from pricing_simulator import PricingSimulator
from pricing_integration import PricingIntegration, track_request_cost, check_request_authorization
import argparse
import socket
from contextlib import contextmanager
from contextlib import asynccontextmanager

# Create the necessary directories - use environment variable for document storage in cloud
DOCUMENT_STORAGE_PATH = os.getenv("DOCUMENT_STORAGE_PATH", "document_storage")
os.makedirs("temp_uploads", exist_ok=True)
os.makedirs(DOCUMENT_STORAGE_PATH, exist_ok=True)

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ultra_api")

# Define document processor placeholder
document_processor = None

# Mock LLM Service for testing
class MockLLMService:
    """Mock LLM service for testing and development"""
    def analyze(self, prompt, llms, ultra_llm, pattern):
        """Mock analysis that returns formatted results"""
        logger.info(f"Mock service analyzing prompt with {len(llms)} models")
        return {
            "status": "success",
            "ultra_response": f"This is a mock response from the Ultra model: {ultra_llm}\n\nAnalysis of your query: {prompt}\n\nPattern used: {pattern}",
            "timing": {
                "total_seconds": 2.5,
                "model_seconds": {model: 1.2 for model in llms}
            }
        }

    # Add method for get_available_models
    async def get_available_models(self):
        return {
            "status": "success",
            "available_models": ["gpt4o", "gpt4turbo", "gpto3mini", "gpto1", "claude37", "claude3opus", "gemini15", "llama3"],
            "errors": {}
        }

    # Add analyze_prompt method that would be awaited
    async def analyze_prompt(self, prompt, models, ultra_model, pattern):
        result = self.analyze(prompt, models, ultra_model, pattern)
        return result

# Config class for runtime settings
class Config:
    """Configuration object to hold runtime settings"""
    use_mock = False
    mock_service = None

# Add response caching
response_cache = TTLCache(maxsize=100, ttl=3600)  # Cache for 1 hour

# Function to generate cache key from request data
def generate_cache_key(prompt, models, ultra_model, pattern):
    """Generate a unique cache key based on request parameters"""
    key_data = f"{prompt}|{','.join(sorted(models))}|{ultra_model}|{pattern}"
    return hashlib.md5(key_data.encode()).hexdigest()

# Initialize performance metrics
performance_metrics = {
    "start_time": datetime.now().isoformat(),
    "requests_processed": 0,
    "documents_processed": 0,
    "total_chunks_processed": 0,
    "total_processing_time": 0,
    "avg_processing_time": 0,
    "max_memory_usage": 0,
    "cache_hits": 0,
    "current_memory_usage_mb": psutil.Process().memory_info().rss / (1024 * 1024)
}

# Initialize metrics history
metrics_history = {
    "timestamps": [],
    "memory_usage": [],
    "requests_processed": [],
    "response_times": []
}

# Initialize processing metrics
requests_processed = 0
processing_times = []
start_time = time.time()

# Define the update_metrics_history function
def update_metrics_history():
    """Update the metrics history with current values"""
    global metrics_history, performance_metrics

    # Update current memory usage
    current_memory = psutil.Process().memory_info().rss / (1024 * 1024)  # MB
    performance_metrics["current_memory_usage_mb"] = current_memory

    # Add current values to history
    metrics_history["timestamps"].append(datetime.now().isoformat())
    metrics_history["memory_usage"].append(current_memory)
    metrics_history["requests_processed"].append(performance_metrics["requests_processed"])

    # Calculate average processing time if we have data
    if processing_times:
        performance_metrics["avg_processing_time"] = sum(processing_times) / len(processing_times)

    # Limit history size to prevent memory issues
    max_history = 100
    if len(metrics_history["timestamps"]) > max_history:
        for key in metrics_history:
            metrics_history[key] = metrics_history[key][-max_history:]

# Define port availability check functions
def is_port_available(port):
    """Check if a port is available"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.bind(('localhost', port))
            return True
        except socket.error:
            return False

def find_available_port(start_port):
    """Find an available port starting from start_port"""
    port = start_port
    while not is_port_available(port) and port < start_port + 100:
        port += 1
    return port

# Define document-related models
class DocumentChunk(BaseModel):
    text: str
    relevance: float
    page: Optional[int] = None

class ProcessedDocument(BaseModel):
    id: str
    name: str
    chunks: List[DocumentChunk]
    totalChunks: int
    type: str

class DocumentUploadResponse(BaseModel):
    id: str
    name: str
    size: int
    type: str
    status: str
    message: str

# Define request models for API endpoints
class TokenEstimateRequest(BaseModel):
    prompt: str
    model: str
    requestType: str
    userId: Optional[str] = None

class PricingToggleRequest(BaseModel):
    enabled: bool
    reason: str

class UserAccountRequest(BaseModel):
    userId: str
    tier: str
    initialBalance: float

class AddFundsRequest(BaseModel):
    userId: str
    amount: float
    description: str = "Account deposit"

# Document processor class - simplified version
class UltraDocumentsOptimized:
    def __init__(self):
        """Initialize the optimized document processor"""
        self.cache_enabled = True

        # Create a simple object with the required memory_cache attribute
        class MemoryCacheObject:
            def size(self):
                return 0

        class CacheObject:
            def __init__(self):
                self.memory_cache = MemoryCacheObject()

        self.cache = CacheObject()

    def process_document(self, file_path):
        """Process a document and extract text chunks with mock relevance"""
        try:
            # For demonstration, just return some mock chunks
            chunks = []

            # Handle different file types - safely handle None
            if file_path:
                extension = os.path.splitext(file_path)[1].lower()
            else:
                extension = ""

            # For text files, try to read content
            if file_path and extension in ['.txt', '.md']:
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()

                    # Split into chunks (simplified)
                    lines = content.split('\n')
                    chunk_size = 10  # lines per chunk
                    for i in range(0, len(lines), chunk_size):
                        chunk_text = '\n'.join(lines[i:i+chunk_size])
                        if chunk_text.strip():
                            chunks.append({
                                "text": chunk_text,
                                "relevance": 0.8  # Mock relevance
                            })
                except Exception as e:
                    logger.error(f"Error reading text file: {str(e)}")
                    # Fall back to mock chunks
                    chunks = [{"text": f"Mock content from {file_path}", "relevance": 0.7}]
            else:
                # For other file types, return mock chunks
                chunks = [
                    {"text": f"Mock content from {file_path or 'unknown'} - part 1", "relevance": 0.8},
                    {"text": f"Mock content from {file_path or 'unknown'} - part 2", "relevance": 0.6}
                ]

            return {"chunks": chunks}
        except Exception as e:
            logger.error(f"Error processing document: {str(e)}")
            return {"chunks": [{"text": "Error processing document", "relevance": 0.1}]}

    def process_documents(self, document_data):
        """Process multiple documents"""
        total_chunks = 0
        processed_chunks = []

        for doc in document_data:
            path = doc.get("path", "")
            doc_result = self.process_document(path)
            doc_chunks = doc_result.get("chunks", [])
            total_chunks += len(doc_chunks)
            processed_chunks.extend(doc_chunks)

        return {
            "chunks_processed": total_chunks,
            "chunks": processed_chunks
        }

# Create a pricing integration mock if not available
class MockPricingIntegration:
    def __init__(self):
        self.pricing_enabled = False

    def estimate_request_cost(self, **kwargs):
        return {
            "estimated_cost": 0.01,
            "tier": "free",
            "has_sufficient_balance": True,
            "cost_details": {
                "base_cost": 0.01,
                "markup_cost": 0,
                "discount_amount": 0,
                "feature_costs": {}
            }
        }

    def check_balance(self, user_id):
        return {"balance": 100.0}

    def create_user_account(self, **kwargs):
        return {"user_id": kwargs.get("user_id"), "tier": kwargs.get("tier")}

    def add_funds(self, **kwargs):
        return {"transaction_id": "mock-123", "amount": kwargs.get("amount")}

    def get_user_usage_summary(self, user_id):
        return {"usage": [], "total_cost": 0}

    def get_session_summary(self, session_id):
        return {"session_id": session_id, "total_cost": 0}

# Initialize pricing integration
try:
    from pricing_integration import PricingIntegration
    pricing_integration = PricingIntegration()
except ImportError:
    pricing_integration = MockPricingIntegration()

# Define mock track_request_cost function if not available
async def track_request_cost(price_integration=None, user_id=None, model=None, token_count=0, tokens_used=None, request_type=None, session_id=None):
    """Track token usage cost - mock implementation"""
    if tokens_used is not None:
        token_count = tokens_used

    logger.info(f"Tracking request cost: {token_count} tokens for {model}")
    return {"status": "success", "cost": 0.01}

# Define check_request_authorization function
async def check_request_authorization(price_integration=None, user_id=None, model=None, estimated_tokens=0, request_type=None):
    """Check if a request is authorized based on user balance"""
    if price_integration is None:
        price_integration = pricing_integration

    # Mock implementation - always authorize
    return {
        "authorized": True,
        "reason": "Request authorized",
        "estimated_cost": 0.01,
        "current_balance": 100.0
    }

# Create FastAPI app instance
app = FastAPI(
    title="Ultra Framework API",
    description="API for the Ultra Framework orchestrating multiple LLMs",
    version="1.0.0"
)

# Register exception handlers
register_exception_handlers(app)

# Add error handling middleware
app.middleware("http")(error_handling_middleware)

# Configure CORS
origins = [
    "http://localhost:3000",
    "http://localhost:3001",
    "http://localhost:3002",
    "http://localhost:3003",
    "http://localhost:3004",
    "http://localhost:3005",
    "http://localhost:3009",
    "http://localhost:3010",
]

# Add Vercel domain if available
vercel_url = os.getenv("VERCEL_URL")
if vercel_url:
    origins.append(f"https://{vercel_url}")

# In production, accept requests from any origin
if os.getenv("ENVIRONMENT") == "production":
    origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Import the pattern orchestrator if available
try:
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from ultra_pattern_orchestrator import PatternOrchestrator
    ORCHESTRATOR_AVAILABLE = True
    logger.info("PatternOrchestrator loaded successfully")
except ImportError:
    ORCHESTRATOR_AVAILABLE = False
    logger.warning("PatternOrchestrator not available, using mock implementation")

    # Define a mock PatternOrchestrator if the real one isn't available
    class PatternOrchestrator:
        """Mock implementation of the PatternOrchestrator"""
        def __init__(self, api_keys=None, pattern=None, output_format=None):
            # Store the init parameters
            self.api_keys = api_keys or {}
            self.pattern = pattern or "confidence"
            self.output_format = output_format or "plain"
            self.ultra_model = None
            logger.info(f"Initialized mock PatternOrchestrator with pattern: {self.pattern}")

        async def analyze(self, prompt, llms, ultra_llm, pattern):
            """Basic analyze method - made async for compatibility"""
            logger.info(f"Mock PatternOrchestrator analyzing prompt with {len(llms)} models and pattern: {pattern}")
            return {
                "status": "success",
                "ultra_response": f"Mock response from PatternOrchestrator using {ultra_llm}. The prompt was: {prompt[:50]}...",
                "results": {"ultra": "Mock ultra response"}
            }

        async def orchestrate_full_process(self, prompt):
            """Mock orchestration process"""
            logger.info(f"Mock orchestration process for prompt: {prompt[:30]}...")

            # Simulate model responses
            initial_responses = {
                "gpt4o": f"GPT-4o analysis of: {prompt[:30]}...",
                "claude37": f"Claude 3.7 analysis of: {prompt[:30]}...",
                "gemini15": f"Gemini 1.5 analysis of: {prompt[:30]}..."
            }

            # Simulate ultra response
            ultra_model = self.ultra_model or "gpt4o"
            ultra_response = f"This is a synthesized response from {ultra_model} analyzing multiple perspectives.\n\nAnalysis: {prompt[:50]}..."

            return {
                "status": "success",
                "initial_responses": initial_responses,
                "meta_responses": {},
                "hyper_responses": {},
                "ultra_response": ultra_response
            }

@app.options("/api/analyze")
async def options_analyze():
    return Response(status_code=200)

@app.options("/api/upload-files")
async def options_upload_files():
    return Response(status_code=200)

@app.options("/api/analyze-with-docs")
async def options_analyze_with_docs():
    return Response(status_code=200)

@app.post("/api/analyze")
async def analyze_prompt(request: Request):
    start_time = time.time()

    try:
        data = await request.json()
        prompt = data.get("prompt")
        selected_models = data.get("llms", data.get("selectedModels", []))
        ultra_model = data.get("ultraLLM", data.get("ultraModel"))
        pattern_name = data.get("pattern", "Confidence Analysis")
        options = data.get("options", {})
        user_id = data.get("userId")

        # Validate required fields
        if not prompt:
            raise ValidationError("Prompt is required")
        if not selected_models:
            raise ValidationError("At least one model must be selected")
        if not ultra_model:
            raise ValidationError("Ultra model is required")

        # Update metrics
        performance_metrics["requests_processed"] += 1

        # Check cache for identical request
        cache_key = generate_cache_key(prompt, selected_models, ultra_model, pattern_name)
        cached_response = response_cache.get(cache_key)

        if cached_response and not options.get("bypass_cache", False):
            logger.info(f"Cache hit for prompt: {prompt[:30]}...")
            performance_metrics["cache_hits"] += 1

            # Add cache indicator to response
            cached_response["cached"] = True
            cached_response["cache_time"] = datetime.now().isoformat()

            # Track cost (even for cached responses)
            if user_id and pricing_integration.pricing_enabled:
                await track_request_cost(
                    user_id=user_id,
                    request_type="cached_analyze",
                    model=ultra_model,
                    tokens_used=0,  # No tokens used for cached response
                )

            return cached_response

        logger.info(f"Received analyze request with pattern: {pattern_name}")

        # Use mock service if in mock mode
        if Config.use_mock and Config.mock_service:
            try:
                result = await Config.mock_service.analyze_prompt(
                    prompt=prompt,
                    models=selected_models,
                    ultra_model=ultra_model,
                    pattern=pattern_name
                )

                # Format the result to match expected response structure
                response = {
                    "status": "success",
                    "results": result.get("results", {}),
                    "ultra_response": result.get("ultra_response", ""),
                    "pattern": result.get("pattern", pattern_name)
                }

                # Cache the response
                response_cache[cache_key] = response

                return response
            except Exception as e:
                logger.error(f"Error in mock analyze: {str(e)}")
                raise HTTPException(status_code=500, detail=f"Mock service error: {str(e)}")

        # Map frontend pattern names to backend pattern keys
        pattern_map = {
            "Confidence Analysis": "confidence",
            "Critique": "critique",
            "Gut Check": "gut",
            "Fact Check": "fact_check",
            "Perspective Analysis": "perspective",
            "Scenario Analysis": "scenario"
        }

        pattern_key = pattern_map.get(pattern_name, "confidence")

        # Check authorization if pricing is enabled
        if user_id and pricing_integration.pricing_enabled:
            auth_result = await check_request_authorization(
                user_id=user_id,
                request_type="analyze",
                model=ultra_model,
                estimated_tokens=len(prompt.split()) * 8  # Rough estimate
            )

            if not auth_result["authorized"]:
                return JSONResponse(
                    status_code=402,  # Payment Required
                    content={
                        "status": "error",
                        "code": "insufficient_balance",
                        "message": "Your account balance is insufficient for this request",
                        "details": auth_result.get("details", {})
                    }
                )

        try:
            # Initialize the orchestrator with the selected models and pattern
            try:
                # Note: ultra_model is set as a separate attribute after initialization
                orchestrator = PatternOrchestrator(
                    api_keys={
                        "anthropic": os.getenv("ANTHROPIC_API_KEY"),
                        "openai": os.getenv("OPENAI_API_KEY"),
                        "google": os.getenv("GOOGLE_API_KEY"),
                        "mistral": os.getenv("MISTRAL_API_KEY"),
                        "deepseek": os.getenv("DEEPSEEK_API_KEY"),
                        "cohere": os.getenv("COHERE_API_KEY"),
                    },
                    pattern=pattern_key,
                    output_format="plain"
                )

                # Set the ultra model after initialization
                orchestrator.ultra_model = ultra_model

                # Process the prompt with the orchestrator
                result = await orchestrator.orchestrate_full_process(prompt)

                # Format the result
                response = {
                    "status": "success",
                    "ultra_response": result.get("ultra_response", ""),
                    "results": {
                        model: content for model, content in result.get("initial_responses", {}).items()
                    },
                    "pattern": pattern_name,
                    "processing_time": time.time() - start_time
                }

                # Cache the response
                response_cache[cache_key] = response

                # Track the request cost if pricing is enabled
                if user_id and pricing_integration.pricing_enabled:
                    # Estimate token usage
                    token_count = sum(len(text.split()) * 4 for text in result.get("initial_responses", {}).values())
                    token_count += len(result.get("ultra_response", "").split()) * 4

                    await track_request_cost(
                        user_id=user_id,
                        request_type="analyze",
                        model=ultra_model,
                        tokens_used=token_count
                    )

                return response

            except TypeError as e:
                # Handle specific initialization errors for different API clients
                logger.warning(f"API client initialization error: {str(e)}")

                # Filter selected_models to only include models we can initialize
                working_models = []

                # Check which models we can use based on the error
                if "AsyncClient.__init__() got an unexpected keyword argument 'proxies'" in str(e):
                    logger.warning("Anthropic/Claude client incompatible - removing from available models")
                    working_models = [model for model in selected_models if model != "claude37" and model != "claude3opus"]
                else:
                    # For other errors, assume we can use OpenAI and Gemini (but not Claude)
                    working_models = [model for model in selected_models if not model.startswith("claude")]

                if not working_models:
                    # If no selected models can work, add a default that usually works
                    if "gpt4turbo" not in selected_models:
                        working_models.append("gpt4turbo")
                    if "gemini15" not in selected_models:
                        working_models.append("gemini15")

                # Create custom safe response with working models only
                result = {
                    "initial_responses": {
                        model: f"Response from {model} model. Some models were unavailable due to API initialization errors."
                        for model in working_models
                    },
                    "meta_responses": {
                        model: f"Meta analysis from {model}."
                        for model in working_models
                    },
                    "hyper_responses": {
                        model: f"Hyper analysis from {model}."
                        for model in working_models
                    },
                    "ultra_response": f"This analysis was limited to working models only. The following API client had initialization errors: Claude/Anthropic.\n\nTo fix this issue, you need to update the anthropic library to a compatible version (0.22.0 is not compatible with the current code).\n\nBased on your query: \"{prompt[:100]}...\"\n\nAnalysis: This is a synthesized response from the working models. For a more complete analysis, please fix the API client compatibility issues."
                }

                # Add a note about the error to the response metadata
                response = {
                    "status": "partial_success",
                    "data": {
                        "initial_responses": {model: content for model, content in result.get("initial_responses", {}).items()},
                        "meta_responses": {model: content for model, content in result.get("meta_responses", {}).items()},
                        "hyper_responses": {model: content for model, content in result.get("hyper_responses", {}).items()},
                        "ultra": result.get("ultra_response", "")
                    },
                    "available_models": working_models,
                    "error_info": {
                        "message": "Some API clients failed to initialize",
                        "detail": str(e),
                        "unavailable_models": [model for model in selected_models if model not in working_models]
                    }
                }

                return JSONResponse(content=response)
            except Exception as e:
                # Re-raise if it's not a specific error we're handling
                raise
        except TypeError as e:
            # Handle missing parameter error specifically
            if "got an unexpected keyword argument" in str(e):
                # Return a simulated response for development/testing
                logger.warning(f"Using simulated response due to error: {str(e)}")

                # Create a mock response with the input data
                mock_responses = {model: f"Simulated response from {model}" for model in selected_models}

                return JSONResponse(content={
                    "status": "success",
                    "data": {
                        "initial_responses": mock_responses,
                        "meta_responses": {},
                        "hyper_responses": {},
                        "ultra": f"Simulated Ultra response for prompt: {prompt[:30]}..."
                    },
                    "note": "This is a simulated response due to API configuration issue"
                })
            else:
                # Re-raise any other TypeError
                raise
    except ValidationError as e:
        logger.warning(f"Validation error: {str(e)}")
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": str(e)}
        )
    except Exception as e:
        logger.error(f"Error in analyze_prompt: {str(e)}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": f"An error occurred: {str(e)}"}
        )

@app.post("/api/upload-files")
async def upload_files(
    files: List[UploadFile] = File(...),
):
    start_time = time.time()
    try:
        processed_documents = []
        temp_paths = []

        # Create document processor instance at the beginning
        document_processor = UltraDocumentsOptimized()

        try:
            # First save all files to temp directory
            for file in files:
                file_id = str(uuid.uuid4())
                # Get the file extension
                extension = os.path.splitext(file.filename)[1].lower()

                # Create a temporary file with the same extension
                with tempfile.NamedTemporaryFile(delete=False, suffix=extension, dir="temp_uploads") as temp_file:
                    # Write the uploaded file content to the temp file
                    content = await file.read()
                    temp_file.write(content)
                    temp_paths.append((temp_file.name, file_id, file.filename, file.content_type or extension[1:].upper()))

            # Create processed documents
            for temp_path, file_id, filename, content_type in temp_paths:
                # Use the stub document processor to create mock chunks
                processed_doc = document_processor.process_document(temp_path)

                # Convert to the expected format
                doc_chunks = []
                for chunk in processed_doc["chunks"]:
                    # Create document chunk with mock relevance
                    doc_chunks.append(DocumentChunk(
                        text=chunk["text"],
                        relevance=chunk["relevance"],
                        page=None
                    ))

                processed_documents.append(ProcessedDocument(
                    id=file_id,
                    name=filename,
                    chunks=doc_chunks,
                    totalChunks=len(doc_chunks),
                    type=content_type
                ))

                # Update performance metrics
                performance_metrics["documents_processed"] += 1
                performance_metrics["total_chunks_processed"] += len(doc_chunks)

        except Exception as e:
            # Log the specific error for debugging
            print(f"Error processing files: {str(e)}")
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"Error processing files: {str(e)}")

        finally:
            # Clean up the temp files
            for temp_path, _, _, _ in temp_paths:
                if os.path.exists(temp_path):
                    try:
                        os.unlink(temp_path)
                    except:
                        pass

        # Update performance metrics
        processing_time = time.time() - start_time
        performance_metrics["total_processing_time"] += processing_time
        performance_metrics["max_memory_usage"] = max(
            performance_metrics["max_memory_usage"],
            psutil.Process().memory_info().rss / (1024 * 1024)  # MB
        )

        return {
            "status": "success",
            "documents": processed_documents,
            "processing_time": processing_time
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/analyze-with-docs")
async def analyze_with_docs(
    background_tasks: BackgroundTasks,
    prompt: str = Form(...),
    selectedModels: str = Form(...),
    ultraModel: str = Form(...),
    files: List[UploadFile] = File([]),
    pattern: str = Form("Confidence Analysis"),
    options: str = Form("{}"),
    userId: str = Form(None)
):
    """Process documents and analyze them with models"""
    return {"status": "success", "message": "Document analysis complete"}

@app.get("/api/status")
async def check_status():
    try:
        return {"status": "operational"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/metrics")
async def get_metrics():
    """Get current performance metrics"""
    try:
        global performance_metrics

        # Add current memory usage and update history
        update_metrics_history()

        # Add cache stats if available
        cache_stats = {}
        document_processor = UltraDocumentsOptimized()
        if document_processor and document_processor.cache_enabled:
            memory_cache_size = document_processor.cache.memory_cache.size()
            cache_stats["memory_cache_size"] = memory_cache_size

        # Calculate uptime
        start_time = datetime.fromisoformat(performance_metrics["start_time"])
        uptime_seconds = (datetime.now() - start_time).total_seconds()

        # Prepare detailed metrics response
        metrics = {
            **performance_metrics,
            "uptime_seconds": uptime_seconds,
            "cache_stats": cache_stats
        }

        return metrics
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting metrics: {str(e)}")

@app.get("/api/metrics/history")
async def get_metrics_history():
    """Get historical metrics data for time-series visualization"""
    try:
        global metrics_history

        # Update history with current values before returning
        update_metrics_history()

        return metrics_history
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting metrics history: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint for monitoring and Docker health checks"""
    try:
        # Check if we can initialize our main classes as a basic health check
        from ultra_pattern_orchestrator import PatternOrchestrator

        # Get memory usage
        memory_usage = psutil.Process().memory_info().rss / (1024 * 1024)  # MB

        # Get CPU usage
        cpu_percent = psutil.Process().cpu_percent(interval=0.1)

        # Check disk space
        disk_usage = psutil.disk_usage('/')

        # System information
        system_info = {
            "platform": sys.platform,
            "python_version": sys.version,
            "cpu_count": multiprocessing.cpu_count(),
            "memory_total": psutil.virtual_memory().total / (1024 * 1024 * 1024),  # GB
        }

        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "version": "1.0.0",
            "system_info": system_info,
            "metrics": {
                "memory_usage_mb": memory_usage,
                "cpu_percent": cpu_percent,
                "disk_usage_percent": disk_usage.percent,
                "requests_processed": performance_metrics["requests_processed"],
                "documents_processed": performance_metrics["documents_processed"],
                "avg_processing_time": performance_metrics["avg_processing_time"]
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Service unhealthy: {str(e)}")

# Health check endpoint
@app.get("/api/health", tags=["Health"])
async def health_check():
    """
    Health check endpoint to verify the API is running correctly.
    This is used for monitoring and load balancer health checks.
    """
    return {
        "status": "ok",
        "version": app.version,
        "timestamp": datetime.now().isoformat(),
        "environment": os.getenv("ENVIRONMENT", "development")
    }

# Token usage estimate endpoint
@app.post("/api/estimate-tokens")
async def estimate_tokens(request: TokenEstimateRequest):
    # Simple token estimation logic
    estimated_tokens = len(request.prompt.split()) * 4  # Simple estimate

    # Get cost estimate if pricing is enabled
    cost_estimate = {"cost": 0, "details": {}}

    if request.userId:
        estimate = pricing_integration.estimate_request_cost(
            user_id=request.userId,
            model=request.model,
            estimated_tokens=estimated_tokens,
            request_type=request.requestType
        )

        cost_estimate = {
            "cost": estimate["estimated_cost"],
            "tier": estimate["tier"],
            "has_sufficient_balance": estimate.get("has_sufficient_balance", True),
            "details": {
                "base_cost": estimate["cost_details"].get("base_cost", 0),
                "markup": estimate["cost_details"].get("markup_cost", 0),
                "discount": estimate["cost_details"].get("discount_amount", 0),
                "features": estimate["cost_details"].get("feature_costs", {})
            }
        }

    return {
        "prompt_length": len(request.prompt),
        "estimated_tokens": estimated_tokens,
        "model": request.model,
        "requestType": request.requestType,
        "pricing_enabled": pricing_integration.pricing_enabled,
        "cost_estimate": cost_estimate
    }

# Document processing endpoint with pricing integration
@app.post("/api/process-documents-with-pricing")
async def process_documents_with_pricing(
    background_tasks: BackgroundTasks,
    prompt: str = Form(...),
    selectedModels: str = Form(...),
    ultraModel: str = Form(...),
    files: List[UploadFile] = File([]),
    pattern: str = Form("Confidence Analysis"),
    options: str = Form("{}"),
    userId: str = Form(None)
):
    global requests_processed, processing_times
    start_process_time = time.time()

    # Generate a session ID for this request
    session_id = str(uuid.uuid4())

    # Parse JSON strings
    try:
        selected_models = json.loads(selectedModels)
        options_dict = json.loads(options)
    except json.JSONDecodeError:
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "Invalid JSON in selectedModels or options"}
        )

    # Estimate token usage and check authorization if pricing is enabled
    # Calculate a base token estimate from prompt + additional for each document
    base_token_estimate = len(prompt.split()) * 4
    doc_token_estimate = sum([5000 for _ in files])  # Rough estimate per document
    estimated_tokens = base_token_estimate + doc_token_estimate

    if pricing_integration.pricing_enabled and userId:
        auth_result = await check_request_authorization(
            price_integration=pricing_integration,
            user_id=userId,
            model=ultraModel,
            estimated_tokens=estimated_tokens,
            request_type="document_processing"
        )

        if not auth_result["authorized"]:
            return JSONResponse(
                status_code=402,  # Payment Required
                content={
                    "status": "error",
                    "message": auth_result["reason"],
                    "estimated_cost": auth_result.get("estimated_cost", 0),
                    "current_balance": auth_result.get("current_balance", 0)
                }
            )

    try:
        # Process documents
        document_data = []
        for file in files:
            content = await file.read()
            # Save the file temporarily
            temp_path = f"temp/{file.filename}"
            os.makedirs(os.path.dirname(temp_path), exist_ok=True)

            with open(temp_path, "wb") as f:
                f.write(content)

            # Process the document
            document_data.append({
                "name": file.filename,
                "path": temp_path,
                "size": len(content)
            })

        # Process the request with your documents
        # This is a placeholder for your actual document processing logic
        processed_docs = document_processor.process_documents(document_data)

        # Process the query with documents
        response = {
            "result": f"Processed prompt with {len(files)} documents: {prompt[:50]}...",
            "selectedModels": selected_models,
            "ultraModel": ultraModel,
            "pattern": pattern,
            "document_metadata": {
                "file_count": len(files),
                "chunks_used": processed_docs.get("chunks_processed", 0),
                "total_tokens": estimated_tokens
            },
            "processing_time": time.time() - start_process_time,
            "session_id": session_id
        }

        # Update metrics
        requests_processed += 1
        processing_time = time.time() - start_process_time
        processing_times.append(processing_time)

        # Track token usage in background
        background_tasks.add_task(
            track_token_usage_background,
            user_id=userId or "anonymous",
            model=ultraModel,
            token_count=estimated_tokens,
            request_type="document_processing",
            session_id=session_id
        )

        # Clean up temp files in background
        background_tasks.add_task(cleanup_temp_files, [doc["path"] for doc in document_data])

        return {
            "status": "success",
            "result": response,
            "document_metadata": response["document_metadata"],
            "processing_time": processing_time
        }

    except Exception as e:
        logger.error(f"Error processing documents: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"status": "error", "message": str(e)}
        )

# Pricing toggle endpoint (admin only)
@app.post("/api/admin/pricing/toggle")
async def toggle_pricing(request: PricingToggleRequest):
    # In a real system, you would add authentication/authorization here
    prev_state = pricing_integration.pricing_enabled
    pricing_integration.pricing_enabled = request.enabled

    logger.info(f"Pricing {'enabled' if request.enabled else 'disabled'}, reason: {request.reason}")

    return {
        "status": "success",
        "pricing_enabled": pricing_integration.pricing_enabled,
        "previous_state": prev_state,
        "message": f"Pricing has been {'enabled' if request.enabled else 'disabled'}"
    }

# User account endpoints
@app.post("/api/user/create")
async def create_user(request: UserAccountRequest):
    result = pricing_integration.create_user_account(
        user_id=request.userId,
        tier=request.tier,
        initial_balance=request.initialBalance
    )

    if "error" in result:
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": result["error"]}
        )

    return {
        "status": "success",
        "user": result
    }

@app.post("/api/user/add-funds")
async def add_funds(request: AddFundsRequest):
    result = pricing_integration.add_funds(
        user_id=request.userId,
        amount=request.amount,
        description=request.description
    )

    if "error" in result:
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": result["error"]}
        )

    return {
        "status": "success",
        "transaction": result
    }

@app.get("/api/user/{user_id}/balance")
async def get_user_balance(user_id: str):
    result = pricing_integration.check_balance(user_id)

    if "error" in result:
        return JSONResponse(
            status_code=404,
            content={"status": "error", "message": result["error"]}
        )

    return {
        "status": "success",
        "balance": result
    }

@app.get("/api/user/{user_id}/usage")
async def get_user_usage(user_id: str):
    result = pricing_integration.get_user_usage_summary(user_id)

    if "error" in result and "No usage data" not in result["error"]:
        return JSONResponse(
            status_code=404,
            content={"status": "error", "message": result["error"]}
        )

    return {
        "status": "success",
        "usage": result
    }

@app.get("/api/session/{session_id}")
async def get_session(session_id: str):
    result = pricing_integration.get_session_summary(session_id)

    if "error" in result:
        return JSONResponse(
            status_code=404,
            content={"status": "error", "message": result["error"]}
        )

    return {
        "status": "success",
        "session": result
    }

# Background tasks
async def track_token_usage_background(
    user_id: str,
    model: str,
    token_count: int,
    request_type: str = "completion",
    session_id: Optional[str] = None
):
    """Background task to track token usage"""
    await track_request_cost(
        price_integration=pricing_integration,
        user_id=user_id,
        model=model,
        token_count=token_count,
        request_type=request_type,
        session_id=session_id
    )

def cleanup_temp_files(file_paths: List[str]):
    """Clean up temporary files"""
    for path in file_paths:
        try:
            if os.path.exists(path):
                os.remove(path)
        except Exception as e:
            logger.error(f"Error removing temp file {path}: {e}")

@app.get("/api/system/health")
async def get_health():
    """Health check endpoint to verify the API is running"""
    try:
        # Create a proper JSON response
        health_data = json.dumps({
            "status": "healthy",
            "uptime": time.time() - start_time,
            "timestamp": datetime.now().isoformat(),
            "version": "1.0.0",
            "memory_usage_mb": performance_metrics["current_memory_usage_mb"],
            "requests_processed": performance_metrics["requests_processed"]
        })

        # Return an explicit Response with hard-coded content length
        return Response(
            content=health_data,
            media_type="application/json",
            headers={"Content-Length": str(len(health_data))}
        )
    except Exception as e:
        logger.error(f"Error in health check: {str(e)}")
        error_msg = json.dumps({"status": "error", "message": str(e)})
        return Response(
            content=error_msg,
            media_type="application/json",
            status_code=500,
            headers={"Content-Length": str(len(error_msg))}
        )

@app.get("/api/test")
async def test_api():
    return {"status": "success", "message": "API is working correctly!"}

@app.get("/api/available-models")
async def get_available_models():
    """Check which LLM models are available for use"""
    # Use mock service if in mock mode
    if Config.use_mock and Config.mock_service:
        return await Config.mock_service.get_available_models()

    available_models = []
    error_messages = {}

    # Check if OpenAI models are available
    try:
        # Just validate that we can initialize the client
        from openai import AsyncOpenAI
        test_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        # If no error, add OpenAI models
        available_models.extend(["gpt4o", "gpto1", "gpto3mini", "gpt4turbo"])
    except Exception as e:
        error_messages["openai"] = str(e)

    # Check if Anthropic/Claude models are available
    try:
        # Just validate that we can initialize the client
        from anthropic import AsyncAnthropic
        test_client = AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        # If no error, add Claude models
        available_models.extend(["claude37", "claude3opus"])
    except Exception as e:
        error_messages["anthropic"] = str(e)

    # Check if Google/Gemini models are available
    try:
        # Just validate that we can configure the API
        import google.generativeai as genai
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        # If no error, add Gemini model
        available_models.append("gemini15")
    except Exception as e:
        error_messages["google"] = str(e)

    # Llama is a local model - assume it's always available
    available_models.append("llama3")

    # Return the list of available models and any errors
    return {
        "status": "success",
        "available_models": available_models,
        "errors": error_messages
    }

@app.get("/api/sentry-debug")
async def trigger_error():
    """
    Test endpoint to verify Sentry integration
    This endpoint deliberately raises an exception that will be caught by Sentry
    """
    logger.info("Testing Sentry integration by triggering a deliberate error")
    division_by_zero = 1 / 0
    return {"status": "This will never be returned"}

# Document processing endpoint
@app.post("/api/upload-document", response_model=DocumentUploadResponse)
async def upload_document(file: UploadFile = File(...)):
    """
    Upload a document file to be analyzed by the AI.
    Supports PDF, TXT, MD, DOC, and DOCX files.
    """
    try:
        # Generate a unique ID for the document
        document_id = str(uuid.uuid4())

        # Get file extension and validate file type
        file_ext = os.path.splitext(file.filename)[1].lower()
        allowed_extensions = ['.pdf', '.txt', '.md', '.doc', '.docx']

        if file_ext not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file type. Allowed types: {', '.join(allowed_extensions)}"
            )

        # Create document storage path with the UUID
        document_dir = os.path.join(DOCUMENT_STORAGE_PATH, document_id)
        os.makedirs(document_dir, exist_ok=True)

        # Save the file to disk
        file_path = os.path.join(document_dir, file.filename)
        with open(file_path, "wb") as f:
            shutil.copyfileobj(file.file, f)

        # Get file size
        file_size = os.path.getsize(file_path)

        # Create a metadata file for the document
        metadata = {
            "id": document_id,
            "original_filename": file.filename,
            "file_path": file_path,
            "file_size": file_size,
            "file_type": file_ext,
            "upload_timestamp": datetime.now().isoformat(),
            "processing_status": "pending"
        }

        # Save metadata
        with open(os.path.join(document_dir, "metadata.json"), "w") as f:
            json.dump(metadata, f, indent=2)

        # Process the document in the background (don't wait for completion)
        try:
            # Use the document processor if available, otherwise just log
            if 'document_processor' in globals():
                document_processor.process_document(file_path)
                processing_status = "processing"
            else:
                # For mock operation
                logger.info(f"Document uploaded: {file.filename} (ID: {document_id})")
                processing_status = "ready"  # Simulate ready for mock

            # Update processing status
            metadata["processing_status"] = processing_status
            with open(os.path.join(document_dir, "metadata.json"), "w") as f:
                json.dump(metadata, f, indent=2)

        except Exception as e:
            logger.error(f"Error processing document {document_id}: {str(e)}")
            # Continue despite processing error - we can try to process later

        # Return document info
        return DocumentUploadResponse(
            id=document_id,
            name=file.filename,
            size=file_size,
            type=file_ext,
            status="uploaded",
            message="Document uploaded successfully"
        )

    except Exception as e:
        # Log the detailed error
        logger.error(f"Error uploading document: {str(e)}")
        logger.error(traceback.format_exc())

        # Re-raise as HTTP exception
        if isinstance(e, HTTPException):
            raise
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")
    finally:
        # Make sure to close the file
        await file.close()

# Document retrieval endpoint
@app.get("/api/documents/{document_id}", response_model=Dict[str, Any])
async def get_document(document_id: str):
    """Get document metadata and status"""
    # Construct the document path
    document_dir = os.path.join(DOCUMENT_STORAGE_PATH, document_id)
    metadata_path = os.path.join(document_dir, "metadata.json")

    # Check if document exists
    if not os.path.exists(metadata_path):
        raise HTTPException(status_code=404, detail="Document not found")

    # Read metadata
    try:
        with open(metadata_path, "r") as f:
            metadata = json.load(f)
        return metadata
    except Exception as e:
        logger.error(f"Error reading document metadata: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving document metadata")

# Document list endpoint
@app.get("/api/documents", response_model=List[Dict[str, Any]])
async def list_documents():
    """List all uploaded documents"""
    documents = []

    # Scan document storage directory
    try:
        for document_id in os.listdir(DOCUMENT_STORAGE_PATH):
            document_dir = os.path.join(DOCUMENT_STORAGE_PATH, document_id)
            metadata_path = os.path.join(document_dir, "metadata.json")

            if os.path.isdir(document_dir) and os.path.exists(metadata_path):
                try:
                    with open(metadata_path, "r") as f:
                        metadata = json.load(f)
                    documents.append(metadata)
                except Exception as e:
                    logger.warning(f"Error reading metadata for document {document_id}: {str(e)}")
    except Exception as e:
        logger.error(f"Error listing documents: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving document list")

    return documents

# Analyze endpoint - modify to handle documents
@app.post("/api/analyze")
async def analyze(request: dict = Body(...)):
    """
    Analyze a prompt using multiple LLMs and an Ultra LLM
    """
    global processing_times
    start_processing = time.time()

    try:
        # Extract request parameters
        prompt = request.get("prompt", "")
        models = request.get("models", ["gpt4o", "gpt4turbo"])
        ultra_model = request.get("ultraModel", "gpt4o")
        pattern = request.get("pattern", "confidence")
        user_id = request.get("userId")
        session_id = request.get("sessionId")

        # Basic validation
        if not prompt.strip():
            raise HTTPException(status_code=400, detail="Prompt cannot be empty")

        # Use cache if available
        cache_key = generate_cache_key(prompt, models, ultra_model, pattern)
        cached_response = response_cache.get(cache_key) if hasattr(response_cache, "get") else None

        if cached_response:
            performance_metrics["cache_hits"] += 1
            logger.info(f"Cache hit for prompt: {prompt[:30]}...")
            end_processing = time.time()
            processing_time = end_processing - start_processing
            processing_times.append(processing_time)
            performance_metrics["requests_processed"] += 1
            update_metrics_history()

            return cached_response

        # Check if we should use mock
        if Config.use_mock and Config.mock_service:
            logger.info(f"Using mock service for prompt: {prompt[:30]}...")
            # Use the async analyze_prompt for consistency
            result = await Config.mock_service.analyze_prompt(prompt, models, ultra_model, pattern)

            # Cache the result
            if hasattr(response_cache, "update"):
                response_cache[cache_key] = result

            # Update metrics
            end_processing = time.time()
            processing_time = end_processing - start_processing
            processing_times.append(processing_time)
            performance_metrics["requests_processed"] += 1
            update_metrics_history()

            return result

        # Use orchestrator if available
        if ORCHESTRATOR_AVAILABLE:
            logger.info(f"Using pattern orchestrator for prompt: {prompt[:30]}...")
            # Get or create orchestrator
            orchestrator = PatternOrchestrator()
            orchestrator.ultra_model = ultra_model

            # Use the full process for better results
            result = await orchestrator.orchestrate_full_process(prompt)

            # Cache the result
            if hasattr(response_cache, "update"):
                response_cache[cache_key] = result

            # Update metrics
            end_processing = time.time()
            processing_time = end_processing - start_processing
            processing_times.append(processing_time)
            performance_metrics["requests_processed"] += 1
            update_metrics_history()

            return result

        # Fallback to basic analysis
        raise HTTPException(status_code=500, detail="No analysis service available")

    except Exception as e:
        logger.error(f"Error in analyze endpoint: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Analysis error: {str(e)}")

async def process_document_context(document_ids: List[str], query: str) -> str:
    """
    Process and retrieve relevant content from documents based on the query.
    Returns a string of relevant content to include in the prompt.
    """
    context_parts = []

    for doc_id in document_ids:
        document_dir = os.path.join(DOCUMENT_STORAGE_PATH, doc_id)
        metadata_path = os.path.join(document_dir, "metadata.json")

        if not os.path.exists(metadata_path):
            logger.warning(f"Document {doc_id} not found")
            continue

        try:
            # Read metadata
            with open(metadata_path, "r") as f:
                metadata = json.load(f)

            file_path = metadata.get("file_path")
            if not file_path or not os.path.exists(file_path):
                logger.warning(f"File not found for document {doc_id}")
                continue

            # Simple document processing - just read the file content
            # In a real implementation, you'd use semantic search or chunking
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()

            # For simplicity, just use the first 1000 characters
            # In production, this would use proper document processing and relevance ranking
            doc_context = f"Document: {metadata.get('original_filename', 'Unnamed document')}\n{content[:3000]}\n\n"
            context_parts.append(doc_context)

        except Exception as e:
            logger.error(f"Error processing document {doc_id}: {str(e)}")
            # Continue with other documents

    # Combine all document contexts
    return "\n".join(context_parts)

# Document chunking endpoints
@app.post("/api/create-document-session")
async def create_document_session(request: Request):
    """Initialize a chunked document upload session"""
    try:
        # Parse request data
        body = await request.json()
        file_name = body.get("fileName")
        file_size = body.get("fileSize")
        total_chunks = body.get("totalChunks")
        session_id = body.get("sessionId")

        # Validate inputs
        if not all([file_name, file_size, total_chunks, session_id]):
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "Missing required parameters"}
            )

        # Create session directory
        session_dir = os.path.join("temp_uploads", session_id)
        os.makedirs(session_dir, exist_ok=True)

        # Save session metadata
        metadata = {
            "file_name": file_name,
            "file_size": file_size,
            "total_chunks": total_chunks,
            "received_chunks": 0,
            "created_at": datetime.now().isoformat(),
            "status": "initialized"
        }

        with open(os.path.join(session_dir, "metadata.json"), "w") as f:
            json.dump(metadata, f)

        return JSONResponse(
            content={"success": True, "message": "Upload session created", "session_id": session_id}
        )

    except Exception as e:
        logger.error(f"Error creating document session: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"Error: {str(e)}"}
        )

@app.post("/api/upload-document-chunk")
async def upload_document_chunk(
    chunk: UploadFile = File(...),
    sessionId: str = Form(...),
    chunkIndex: str = Form(...),
    fileName: str = Form(...)
):
    """Upload a chunk of a document"""
    try:
        # Verify session exists
        session_dir = os.path.join("temp_uploads", sessionId)
        if not os.path.exists(session_dir):
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "Session not found"}
            )

        # Verify metadata
        metadata_path = os.path.join(session_dir, "metadata.json")
        if not os.path.exists(metadata_path):
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "Session metadata not found"}
            )

        # Load metadata
        with open(metadata_path, "r") as f:
            metadata = json.load(f)

        # Save the chunk
        chunk_index = int(chunkIndex)
        chunk_path = os.path.join(session_dir, f"chunk_{chunk_index}")

        # Write chunk to file
        with open(chunk_path, "wb") as f:
            chunk_content = await chunk.read()
            f.write(chunk_content)

        # Update metadata
        metadata["received_chunks"] = metadata.get("received_chunks", 0) + 1
        with open(metadata_path, "w") as f:
            json.dump(metadata, f)

        return JSONResponse(content={
            "success": True,
            "message": f"Chunk {chunk_index} received",
            "received": metadata["received_chunks"],
            "total": metadata["total_chunks"]
        })

    except Exception as e:
        logger.error(f"Error uploading chunk: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"Error: {str(e)}"}
        )
    finally:
        await chunk.close()

@app.post("/api/finalize-document-upload")
async def finalize_document_upload(request: Request):
    """Finalize a chunked document upload by combining all chunks"""
    try:
        # Parse request data
        body = await request.json()
        session_id = body.get("sessionId")
        file_name = body.get("fileName")

        # Validate inputs
        if not all([session_id, file_name]):
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "Missing required parameters"}
            )

        # Verify session exists
        session_dir = os.path.join("temp_uploads", session_id)
        if not os.path.exists(session_dir):
            return JSONResponse(
                status_code=400,
                content={"success": False, "message": "Session not found"}
            )

        # Load metadata
        metadata_path = os.path.join(session_dir, "metadata.json")
        with open(metadata_path, "r") as f:
            metadata = json.load(f)

        # Verify all chunks received
        received_chunks = metadata.get("received_chunks", 0)
        total_chunks = metadata.get("total_chunks", 0)

        if received_chunks != total_chunks:
            return JSONResponse(
                status_code=400,
                content={
                    "success": False,
                    "message": f"Not all chunks received ({received_chunks}/{total_chunks})"
                }
            )

        # Generate a unique ID for the document
        document_id = str(uuid.uuid4())

        # Create document storage path with the UUID
        document_dir = os.path.join(DOCUMENT_STORAGE_PATH, document_id)
        os.makedirs(document_dir, exist_ok=True)

        # Combine all chunks into the final file
        file_path = os.path.join(document_dir, file_name)
        with open(file_path, "wb") as outfile:
            for i in range(total_chunks):
                chunk_path = os.path.join(session_dir, f"chunk_{i}")
                if os.path.exists(chunk_path):
                    with open(chunk_path, "rb") as chunk_file:
                        outfile.write(chunk_file.read())

        # Get file size
        file_size = os.path.getsize(file_path)

        # Create document metadata
        doc_metadata = {
            "id": document_id,
            "original_filename": file_name,
            "file_path": file_path,
            "file_size": file_size,
            "file_type": os.path.splitext(file_name)[1].lower(),
            "upload_timestamp": datetime.now().isoformat(),
            "processing_status": "ready",
            "chunked_upload": True,
            "upload_session_id": session_id
        }

        # Save document metadata
        with open(os.path.join(document_dir, "metadata.json"), "w") as f:
            json.dump(doc_metadata, f, indent=2)

        # Clean up the session directory
        try:
            shutil.rmtree(session_dir)
        except Exception as e:
            logger.warning(f"Could not clean up session directory: {str(e)}")

        return JSONResponse(content={
            "success": True,
            "message": "Document upload completed successfully",
            "id": document_id,
            "name": file_name,
            "size": file_size,
            "type": os.path.splitext(file_name)[1].lower(),
            "status": "uploaded"
        })

    except Exception as e:
        logger.error(f"Error finalizing document upload: {str(e)}")
        logger.error(traceback.format_exc())
        return JSONResponse(
            status_code=500,
            content={"success": False, "message": f"Error: {str(e)}"}
        )

# Run server
if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run the UltraAI backend server")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind the server to")
    parser.add_argument("--port", type=int, default=8085, help="Port to bind the server to")
    parser.add_argument("--reload", action="store_true", help="Enable auto-reload on code changes")
    parser.add_argument("--find-port", action="store_true", help="Find an available port if specified port is in use")
    parser.add_argument("--mock", action="store_true", help="Run in mock mode with simulated responses")
    args = parser.parse_args()

    # Set config from arguments
    Config.use_mock = args.mock
    if Config.use_mock:
        try:
            from mock_llm_service import MockLLMService
            Config.mock_service = MockLLMService()
            print(" Running in MOCK MODE - all responses will be simulated")
        except ImportError:
            print(" Mock service module not found. Please create mock_llm_service.py first.")
            sys.exit(1)

    # Create temp directories for file uploads and document processing
    os.makedirs("temp_uploads", exist_ok=True)
    os.makedirs("temp", exist_ok=True)
    os.makedirs("logs", exist_ok=True)

    port = args.port

    # If find-port is specified and the port is not available, find an available port
    if args.find_port and not is_port_available(port):
        original_port = port
        port = find_available_port(original_port)
        logger.info(f"Port {original_port} is in use, using port {port} instead")

    # Add CORS origins for the actual port we're using
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[f"http://localhost:{port}"] +
                     [f"http://localhost:{i}" for i in range(3000, 3020)],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    print(f"Starting server on http://{args.host}:{port}")

    # Run the server
    uvicorn.run(
        "main:app",
        host=args.host,
        port=port,
        reload=args.reload
    )