# Ultra Environment Configuration
# Copy this file to .env to set your environment variables

# ------------------------------
# Core Configuration
# ------------------------------
PORT=8000
NODE_ENV=development
LOG_LEVEL=info
DEBUG=false

# ------------------------------
# Docker Compose Configuration
# ------------------------------
# Set to 'development' or 'production'
BUILD_TARGET=development
# Docker image tag
TAG=latest
# Compose project name
COMPOSE_PROJECT_NAME=ultra
# Restart policy (no, on-failure, always, unless-stopped)
RESTART_POLICY=unless-stopped

# ------------------------------
# Security
# ------------------------------
# Generate a secure random string for signing JWT tokens
JWT_SECRET=your-jwt-secret-here
# CORS settings
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:3009
# Allowed external domains for URL validation (prevents SSRF attacks)
ALLOWED_EXTERNAL_DOMAINS=api.openai.com,api.anthropic.com,generativelanguage.googleapis.com,api.mistral.ai,api.cohere.ai

# ------------------------------
# LLM API Keys
# ------------------------------
# OpenAI API keys (Required for GPT models)
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_ORG_ID=org-xxxxxxxxxxxxxxxxxxxx

# Anthropic API key (Required for Claude models)
ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Google API key (Required for Gemini models)
GOOGLE_API_KEY=AIza-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Ollama settings (for local models)
# When using Docker Compose, this should point to the Ollama service
OLLAMA_BASE_URL=http://ollama:11434
ENABLE_OLLAMA=false

# ------------------------------
# Database Configuration
# ------------------------------
# For Docker Compose, these should match the service names in docker-compose.yml
DB_HOST=postgres
DB_PORT=5432
DB_USER=postgres
DB_PASSWORD=postgres_password
DB_NAME=ultra
# Database connection string (built from the above variables)
# DATABASE_URL will be auto-constructed if not provided
# DATABASE_URL=postgresql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}

# ------------------------------
# Caching Configuration
# ------------------------------
ENABLE_CACHE=true
CACHE_TTL=3600
MAX_CACHE_ITEMS=1000
# Redis Configuration (for Docker Compose)
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=redis_password
REDIS_DB=0
# REDIS_URL will be auto-constructed if not provided
# REDIS_URL=redis://:${REDIS_PASSWORD}@${REDIS_HOST}:${REDIS_PORT}/${REDIS_DB}

# ------------------------------
# Performance Settings
# ------------------------------
# Maximum concurrent requests for each LLM provider
MAX_CONCURRENT_OPENAI=10
MAX_CONCURRENT_ANTHROPIC=5
MAX_CONCURRENT_GOOGLE=8
MAX_CONCURRENT_OLLAMA=2

# Maximum request timeout in milliseconds
REQUEST_TIMEOUT=60000

# ------------------------------
# Feature Flags
# ------------------------------
# Use mock LLM responses for testing without API keys
USE_MOCK=true
# Auto-register all LLM providers even without API keys (they'll work in mock mode)
AUTO_REGISTER_PROVIDERS=true
# Enable document processing features
ENABLE_DOCUMENT_PROCESSING=true
# Enable pricing calculator and estimations
ENABLE_PRICING=false
# Enable authentication and user accounts
ENABLE_AUTH=false

# ------------------------------
# Docker Model Runner Configuration
# ------------------------------
# Enable Docker Model Runner integration
ENABLE_MODEL_RUNNER=false
# Docker Model Runner API URL
MODEL_RUNNER_PORT=8080
# Default model to use for local inference
DEFAULT_MODEL=phi3:mini
# Available models (comma-separated)
AVAILABLE_MODELS=phi3:mini,llama3:8b,mistral:7b
# Use local models when internet is unavailable
USE_LOCAL_MODELS_WHEN_OFFLINE=true
# Timeout for local model requests (milliseconds)
LOCAL_MODEL_TIMEOUT=60000
# GPU support (true for hardware acceleration)
GPU_ENABLED=false
# Maximum cache size for models in MB
MODEL_CACHE_SIZE_MB=8192
# Log level for Model Runner
MODEL_RUNNER_LOG_LEVEL=info

# ------------------------------
# Monitoring and Analytics
# ------------------------------
ENABLE_TELEMETRY=false
# Sentry DSN for error tracking and monitoring
SENTRY_DSN=
ENABLE_PERFORMANCE_METRICS=true

# Prometheus metrics configuration
ENABLE_METRICS=true
METRICS_PORT=8081
SYSTEM_METRICS_INTERVAL=15
# Enable logging of request details (method, path, status, duration)
ENABLE_REQUEST_LOGGING=true

# ------------------------------
# API Endpoints (Optional)
# ------------------------------
# Custom API endpoints for LLM providers (use when connecting to proxies or alternate endpoints)
# OPENAI_API_ENDPOINT=https://api.openai.com/v1
# ANTHROPIC_API_ENDPOINT=https://api.anthropic.com
# MISTRAL_API_ENDPOINT=https://api.mistral.ai/v1
# COHERE_API_ENDPOINT=https://api.cohere.ai/v1

# ------------------------------
# Frontend Configuration
# ------------------------------
# Frontend service port
FRONTEND_PORT=3009
# Backend API URL as seen from the frontend container
VITE_API_BASE_URL=http://localhost:8000
# In Docker Compose, use the service name
VITE_API_SERVICE_URL=http://backend:8000
